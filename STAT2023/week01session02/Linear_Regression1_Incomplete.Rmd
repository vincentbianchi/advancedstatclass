---
title: "Linear Regression in R"
author: "Zifeng Zhao"
date: "week 01 session 02"
output: html_document
editor_options: 
  chunk_output_type: console
---


In this session, we will practice implementing basic linear regression in `R` w.r.t. model estimation, model evaluation (in-sample) and model deployment.

## 1. Read in data
Let's read in the data stored in `UsedCar.csv`.

**Exercise 0:** Read in UsedCar.csv and store it in an R object named `total_data`.
```{r Exercise 0}

total_data <- read.csv("./UsedCar.csv", header=T, stringsAsFactors=T)

```

We can explore the data briefly.
```{r chunk2}
## We can take a look at the first couple of data entries.
head(total_data, n=2)
## str() gives the structure of the data and date type of each column
str(total_data)
## summary() gives the summary statistics of each column
summary(total_data)
## How many observations do we have?
dim(total_data)[1]
## How many different variables do we have?
dim(total_data)[2]
names(total_data)
## We can use table() to explore the number of levels for each variable
table(total_data$Fuel_Type)
```

**Exercise 1:** How many levels are in the variable `Doors`? Should we treat it as a numerical or categorical variable? How many levels are in the predictor `Automatic` and `Metallic`?
```{r Exercise 1}
# how many levels in doors, automatic, and metallic
table(total_data$Doors)

table(total_data$Automatic)

table(total_data$Metallic)
```


## 2. Visualization
We can further do some basic visualization to understand the relationship between the dependent variable and predictors.
```{r chunk3}
## Scatterplot
plot(total_data$Mileage, total_data$Price, ylab='Price', xlab='Mileage',
     main='Price v.s. Mileage')

## If the x variable is factor, R will generate side-by-side boxplot

plot(total_data$Fuel_Type, total_data$Price, ylab='Price', xlab='Fuel Type',
     main='Price v.s. Fuel Type')

plot(total_data$Automatic, total_data$Price)
boxplot(total_data$Price~total_data$Automatic) # y then x with boxplot

## Histogram of Price

hist(total_data$Price, main='Histogram of Price', xlab='')

```

**Exercise 2:** Plot Price v.s. Age (Scatterplot) and Price v.s. Automatic (Boxplot)
```{r Exercise 2}

plot(total_data$Age, total_data$Price)

boxplot(total_data$Price~total_data$Automatic) # y then x with boxplot

```

The histogram of Price is clearly right-skewed. Let's do a log-transformation of the original scale Price and store it in a new variable `Log_price`. Let's further recheck the histogram.
```{r chunk4}
## Log transformation

total_data$Log_price <- log(total_data$Price+1)

#create a new variable within the data frame total_data
total_data$Log_price <- log(total_data$Price+1)

## Histogram of log price
hist(total_data$Log_price)

head(total_data$Log_price)

```


## 3. Linear Regression 
### 3.1 Model Estimation
Let's fit a linear regression model based on the data using the function `lm()`.
```{r chunk5}
lm_full <- lm(Log_price~Age+Mileage+Fuel_Type+HP+Metallic+Automatic+CC+Doors+Quarterly_Tax+Weight, data=total_data)
## We can also use the following code for convenience. Make sure there is no additional unused variables provided!!!
# lm_full <- lm(Log_price~., data=total_data[,-1])

names(total_data)
lm_Full <- lm(Log_price~Age +Mileage+Fuel_Type+HP+Automatic+Metallic+CC+Doors+Quarterly_Tax+Weight, data = total_data)

summary(lm_Full)

lm_full1 <- lm(Log_price~., data = total_data[,-1])
summary(lm_full1)

```


Use the `summary()` function in `R` to look at more details about the fitted linear regression. `lm_full$coefficients` returns the estimated coefficient $\hat{\beta}=(\hat{\beta}_1,\hat{\beta}_2,\cdots,\hat{\beta}_p).$ `lm_full$fitted.values` returns the fitted dependent variable $\hat{Y}=\hat{\beta}_0+\hat{\beta}_1X_1+\cdots+\hat{\beta}_pX_p$ based on estimated coefficient $\hat{\beta}.$ 
```{r chunk6}
## Look at summary of lm_full
summary(lm_full)
summary(lm_Full)
## estimated beta
lm_full$coefficients
```

**Exercise 3:** Which predictors are statistically significant?

**Exercise 4:** What is the impact of Age to log Price and thus to Price? 
```{r Exercise 4}

```

### 3.2 Model Evaluation (In-sample Goodness of Fit)
**Exercise 5:** What is $R^2$ of lm_full? 

Let's now manually calculate the $R^2$ of lm_full using the formula discussed in class and verify that it is the same as the one returned by `summary(lm_full)`.
```{r chunk7}
TSS <- sum((total_data$Log_price-mean(total_data$Log_price))^2)
RSS <- sum((total_data$Log_price-lm_full$fitted.values)^2)
## Calculate R^2

TSS <- sum((total_data$Log_price-mean(total_data$Log_price))^2)
RSS <- sum((total_data$Log_price-lm_full$fitted.values)^2)


1-RSS/TSS

```

**Exercise 6:** Is there another way to calculate RSS?
```{r Exercise 6}
sum((lm_full$residuals)^2)
```

Based on the in-sample residuals, we can calculate popular error measures such as MAE, MAPE, RMSE.
```{r chunk8}
## residuals
error <- lm_full$residuals
MAE <- mean(abs(error))
MAPE <- mean(abs(error/total_data$Log_price))*100
print(c(MAE,MAPE))

mean(abs(error))

mean(abs(error/total_data$Log_price))*100


```

**Exercise 7:** Calculate RMSE manually.
```{r Exercise 7}
##

```

We can also use the package `forecast` to generate MAE, MAPE, RMSE automatically
```{r chunk9}
library(forecast)
## first argument is the prediction, second argument is the actual observation

accuracy(lm_full$fitted.values, total_data$Log_price)

# what is predicted to happen (fitted.values), what actually happened
accuracy(lm_full$fitted.values, total_data$Log_price)
```

Note that these values may not be very meaningful as the results on in log-scale. We should instead evaluate MAE, MAPE, RMSE at the the original scale. In other words, let's transform the fitted log Price to Price. How good/accurate is the **in-sample** fit of the linear regression given by lm_full?
```{r chunk10}
## How do we transform the log-scale fitted value to the original scale?

lm_full$fitted.values
exp(9.699939) - 1

total_data$Price[1]

accuracy(exp(lm_full$fitted.values)-1,  total_data$Price)



```



## 4. In-class Exercise (Part of HW1)
Fit a linear regression model of Log_price w.r.t. two predictors Age and Mileage, name it `lm_short`.

**Q1** What is the $R^2$ of `lm_short`? Is it higher or lower than the $R^2$ of `lm_full`?

**Q2** What is the estimated coefficient of `lm_short` for Age and Mileage? How should we interpret it at the log-scale and the original scale?


```{r In-class Exercise}
# lm_short <- 



```

